{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f9bf0a5-57d0-438a-9718-7dd5df2cb0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# import os\n",
    "# import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import pathlib\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8b361c3-e907-4263-9146-7cd6839c698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# checking if gpu is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3379b807-d075-446f-a8fa-6852296db35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),                  # 0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],   # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                         [0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "890755cc-679f-4aeb-9de3-fa6367f475a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for training and testing directory\n",
    "train_path = 'E:\\\\7_Seventh Sem\\AI\\Building Type Classification\\Data set\\Data set\\seg_train'\n",
    "test_path = 'E:\\\\7_Seventh Sem\\AI\\Building Type Classification\\Data set\\Data set\\seg_test'\n",
    "\n",
    "# load training dataset\n",
    "load_train = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "# load test dataset\n",
    "load_test = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path, transform=transformer),\n",
    "    batch_size=12, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7484476-2a78-4647-a01f-b7992060b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apartments', 'Religious buildings', 'individual buildings', 'office buildings', 'shops', 'slums']\n"
     ]
    }
   ],
   "source": [
    "# available categories of buildings\n",
    "root = pathlib.Path(train_path)\n",
    "categories = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "# print all the categories\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77392b3b-0d9e-44de-91bd-6277b7b5499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Output size after convolution filter\n",
    "        # ((w-f+2P)/s) +1\n",
    "\n",
    "        # Input shape= (32,3,150,150)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        #Shape= (32,12,150,150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        #Shape= (32,12,150,150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #Shape= (32,12,150,150)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Reduce the image size be factor 2\n",
    "        #Shape= (32,12,75,75)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=12, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        #Shape= (32,64,75,75)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        #Shape= (32,64,75,75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #Shape= (32,64,75,75)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=75 * 75 * 64, out_features=num_classes)\n",
    "\n",
    "        # Feed forwad function\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "\n",
    "        output = self.pool(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        # Above output will be in matrix form, with shape (32,64,75,75)\n",
    "        output = output.view(-1, 64*75*75)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "291d40f6-9523-4a6f-a5be-edcc9216dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=6).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0d16180-1f14-4cc0-82cc-c15436ab0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optmizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad239d6b-0e57-45d7-ac1a-10931acd9253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521 120\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 15\n",
    "train_count = len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count = len(glob.glob(test_path+'/**/*.jpg'))\n",
    "\n",
    "# get the size of train and test dataset\n",
    "print(train_count, test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0955cb11-d437-4b3e-8c66-c40130eaa1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(3.9411) Train Accuracy: 0.3416506717850288 Test Accuracy: 0.25833333333333336\n",
      "Epoch: 1 Train Loss: tensor(1.8858) Train Accuracy: 0.5969289827255279 Test Accuracy: 0.45\n",
      "Epoch: 2 Train Loss: tensor(1.5471) Train Accuracy: 0.6621880998080614 Test Accuracy: 0.45\n",
      "Epoch: 3 Train Loss: tensor(0.5541) Train Accuracy: 0.8272552783109405 Test Accuracy: 0.525\n",
      "Epoch: 4 Train Loss: tensor(0.3375) Train Accuracy: 0.8809980806142035 Test Accuracy: 0.4083333333333333\n",
      "Epoch: 5 Train Loss: tensor(0.4231) Train Accuracy: 0.8809980806142035 Test Accuracy: 0.5583333333333333\n",
      "Epoch: 6 Train Loss: tensor(0.1316) Train Accuracy: 0.9558541266794626 Test Accuracy: 0.5833333333333334\n",
      "Epoch: 7 Train Loss: tensor(0.1527) Train Accuracy: 0.9424184261036468 Test Accuracy: 0.5166666666666667\n",
      "Epoch: 8 Train Loss: tensor(0.1025) Train Accuracy: 0.9673704414587332 Test Accuracy: 0.49166666666666664\n",
      "Epoch: 9 Train Loss: tensor(0.0620) Train Accuracy: 0.9788867562380038 Test Accuracy: 0.5166666666666667\n",
      "Epoch: 10 Train Loss: tensor(0.0379) Train Accuracy: 0.9904030710172744 Test Accuracy: 0.5416666666666666\n",
      "Epoch: 11 Train Loss: tensor(0.0437) Train Accuracy: 0.9904030710172744 Test Accuracy: 0.5083333333333333\n",
      "Epoch: 12 Train Loss: tensor(0.0307) Train Accuracy: 0.9904030710172744 Test Accuracy: 0.49166666666666664\n",
      "Epoch: 13 Train Loss: tensor(0.0144) Train Accuracy: 0.9980806142034548 Test Accuracy: 0.55\n",
      "Epoch: 14 Train Loss: tensor(0.0087) Train Accuracy: 1.0 Test Accuracy: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model training and saving best model\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    # Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(load_train):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "\n",
    "    train_accuracy = train_accuracy/train_count\n",
    "    train_loss = train_loss/train_count\n",
    "\n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "\n",
    "    test_accuracy = 0.0\n",
    "    for i, (images, labels) in enumerate(load_test):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "\n",
    "    test_accuracy = test_accuracy/test_count\n",
    "\n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss) +\n",
    "          ' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "\n",
    "    # Save the best model\n",
    "    if test_accuracy > best_acc:\n",
    "        torch.save(model.state_dict(), 'best_check.model')\n",
    "        best_acc = test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b3003ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833333333333334"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65c755f7-a24c-4374-986f-b6925e07a5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc): Linear(in_features=360000, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('best_check.model')\n",
    "model = ConvNet(num_classes=6)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6d3832d-23b2-4201-8bf9-5d731c2300cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),  # 0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],  # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                         [0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bd853a8-3630-4afe-b656-fabfc74033e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def prediction(img_path, transformer):\n",
    "\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    image_tensor = transformer(image).float()\n",
    "\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "\n",
    "    input = Variable(image_tensor)\n",
    "\n",
    "    output = model(input)\n",
    "\n",
    "    index = output.data.numpy().argmax()\n",
    "\n",
    "    pred = categories[index]\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e3f3aa1-737f-4f8d-abd4-6d868463ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = 'E:\\\\7_Seventh Sem\\AI\\Building Type Classification\\Data set\\Data set\\seg_predict'\n",
    "\n",
    "images_path = glob.glob(pred_path+'/*.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76ee3048-b7ed-430c-aac2-5490efa57927",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "\n",
    "for i in images_path:\n",
    "    pred_dict[i[i.rfind('/')+1:]] = prediction(i, transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8ef97d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14948/2385486715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# showing image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdefault_path_part\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"apartment.jpg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         data=None, **kwargs):\n\u001b[1;32m-> 2903\u001b[1;33m     __ret = gca().imshow(\n\u001b[0m\u001b[0;32m   2904\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5607\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5609\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5610\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5611\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    698\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[0;32m    699\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m--> 700\u001b[1;33m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0m\u001b[0;32m    701\u001b[0m                             \"float\".format(self._A.dtype))\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "\n",
    "default_path_part = \"E:\\\\7_Seventh Sem\\AI\\Building Type Classification\\Data set\\Data set\\seg_predict\"\n",
    "\n",
    "Image1 = cv2.imread(default_path_part + \"apartment.jpg\")\n",
    "Image2 = cv2.imread(default_path_part + \"individual.jpg\")\n",
    "Image3 = cv2.imread(default_path_part + \"office.jpg\")\n",
    "Image4 = cv2.imread(default_path_part + \"shop.jpg\")\n",
    "Image5 = cv2.imread(default_path_part + \"slum.jpg\")\n",
    "Image6 = cv2.imread(default_path_part + \"temple.jpg\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "# setting values to rows and column variables\n",
    "rows = 3\n",
    "columns = 3\n",
    "\n",
    "# showing image\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(Image1)\n",
    "plt.title(pred_dict[default_path_part + \"apartment.jpg\"])\n",
    "\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(Image2)\n",
    "plt.title(pred_dict[default_path_part + \"individual.jpg\"])\n",
    "\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(Image3)\n",
    "plt.title(pred_dict[default_path_part + \"office.jpg\"])\n",
    "\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "plt.imshow(Image4)\n",
    "plt.title(pred_dict[default_path_part + \"shop.jpg\"])\n",
    "\n",
    "fig.add_subplot(rows, columns, 5)\n",
    "plt.imshow(Image5)\n",
    "plt.title(pred_dict[default_path_part + \"slum.jpg\"])\n",
    "\n",
    "fig.add_subplot(rows, columns, 6)\n",
    "plt.imshow(Image6)\n",
    "plt.title(pred_dict[default_path_part + \"temple.jpg\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994da7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
